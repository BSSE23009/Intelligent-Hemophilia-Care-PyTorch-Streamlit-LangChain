{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8782be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def load_data(data):\n",
    "    my_loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob = '*.pdf',\n",
    "        loader_cls = PyPDFLoader\n",
    "\n",
    "\n",
    "        \n",
    "    )\n",
    "\n",
    "    docs = my_loader.load()\n",
    "    return docs\n",
    "\n",
    "my_data = load_data(\"book\" )\n",
    "my_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings  # or HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def filter_docs(docs: List[Document]) -> List[Document]:\n",
    "    new_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        new_docs.append(Document(page_content=doc.page_content, metadata={\"source\": src}))\n",
    "    return new_docs\n",
    "\n",
    "medical_new_docs = filter_docs(my_data)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "my_chunks = text_splitter.split_documents(medical_new_docs)\n",
    "\n",
    "pc_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pc_api_key)\n",
    "\n",
    "index_name = \"hemophilia-care-ai\"\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(name=index_name, dimension=1536, metric=\"cosine\", spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # or HuggingFaceEmbeddings if preferred\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "small_chunks = text_splitter.split_documents(my_chunks)\n",
    "\n",
    "vectors = []\n",
    "batch_size = 50\n",
    "for i in range(0, len(small_chunks), batch_size):\n",
    "    batch = small_chunks[i:i+batch_size]\n",
    "    vectors = [\n",
    "        {\"id\": f\"doc-{i+j}\", \"values\": embedding.embed_query(doc.page_content), \"metadata\": {\"text\": doc.page_content}}\n",
    "        for j, doc in enumerate(batch)\n",
    "    ]\n",
    "    index.upsert(vectors=vectors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "docsearch_client = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)\n",
    "retriever = docsearch_client.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o\")\n",
    "system_prompt = (\n",
    "    \"You are a knowledgeable and cautious medical assistant. \"\n",
    "    \"Answer patient or clinician questions using ONLY the retrieved context below. \"\n",
    "    \"If the answer is not in the context, say clearly that you don't know. \"\n",
    "    \"Keep answers concise, factual, and medically accurate. \"\n",
    "    \"Limit responses to a maximum of three sentences and avoid speculation.\"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "response = rag_chain.invoke({\"input\" : \"What is hemophilia?\"})\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hemocare_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
